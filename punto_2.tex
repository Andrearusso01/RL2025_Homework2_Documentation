Vision-based control

\begin{enumerate}
  \item We constructed a gazebo world, named \lstinline|empty.world|, inserting an aruco tag and detected it via the \lstinline|aruco_ros| package (link \href{https://github.com/pal-robotics/aruco_ros.git}{here}). With these lines we added the aruco tag:
\begin{lstlisting}[language=html]
     <include>
      <uri>
        model://aruco_tag
      </uri>
      <name>aruco_tag</name>
      <pose>0 -0.707 0.707 0 1.57 0</pose>
    </include>
\end{lstlisting}
  
Then we created a folder \lstinline|gazebo/models|, in the \lstinline|iiwa_description| package of the \lstinline|ros2_iiwa| stack, containing the aruco marker model for gazebo. 
 
We created a new model named \lstinline|aruco_tag| and imported it into a new Gazebo world as a static object in a position that is visible by the camera. Then we saved the new world into the \lstinline|/gazebo/worlds/|folder.
\newline This is the \lstinline|model.sdf| file:
  \begin{lstlisting}[language=html]
  <?xml version="1.0" encoding="UTF-8"?>
<sdf version='1.9'>
  <model name='aruco_tag'>
    <static>true</static>
    <link name='base_link'>
      <visual name='aruco_visual'>
        <geometry>
          <box>
            <size>0.1 0.1 0.01</size>
          </box>
        </geometry>
        <material>
          <diffuse>1 1 1 1</diffuse>
          <specular>0.4 0.4 0.4 1</specular>
          <pbr>
            <metal>
              <albedo_map>model://aruco_tag/aruco-20.png</albedo_map>
            </metal>
          </pbr>
        </material>
      </visual>
    </link>
  </model>
</sdf>
  \end{lstlisting}

And this is the code of our \lstinline|aruco_world|:
  \begin{lstlisting}[language=html]

<?xml version="1.0" ?>
<sdf version="1.4">
<world name="aruco_world">

<physics name="1ms" type="ignored">
    <max_step_size>0.001</max_step_size>
    <real_time_factor>1.0</real_time_factor>
    </physics>
    <plugin
      filename="ignition-gazebo-physics-system"
      name="gz::sim::systems::Physics">
    </plugin>
    <plugin
      filename="ignition-gazebo-user-commands-system"
      name="gz::sim::systems::UserCommands">
    </plugin>
    <plugin
      filename="ignition-gazebo-scene-broadcaster-system"
      name="gz::sim::systems::SceneBroadcaster">
    </plugin>
    <plugin
      filename="ignition-gazebo-contact-system"
      name="gz::sim::systems::Contact">
    </plugin>

    <light type="directional" name="sun">
      <cast_shadows>true</cast_shadows>
      <pose>0 0 10 0 0 0</pose>
      <diffuse>0.8 0.8 0.8 1</diffuse>
      <specular>0.2 0.2 0.2 1</specular>
      <attenuation>
        <range>1000</range>
        <constant>0.9</constant>
        <linear>0.01</linear>
        <quadratic>0.001</quadratic>
      </attenuation>
      <direction>-0.5 0.1 -0.9</direction>
    </light>

    <model name="ground_plane">
      <static>true</static>
      <link name="link">
        <collision name="collision">
          <geometry>
            <plane>
              <normal>0 0 1</normal>
              <size>100 100</size>
            </plane>
          </geometry>
        </collision>
        <visual name="visual">
          <geometry>
            <plane>
              <normal>0 0 1</normal>
              <size>100 100</size>
            </plane>
          </geometry>
          <material>
            <ambient>0.8 0.8 0.8 1</ambient>
            <diffuse>0.8 0.8 0.8 1</diffuse>
            <specular>0.8 0.8 0.8 1</specular>
          </material>
        </visual>
      </link>
    </model>


<include>
  <uri>
    model://aruco_tag/
  </uri>
  <name>aruco_tag</name>
  <pose>-0.32 -0.83 0.43 2.54 1.57 3.14</pose>
</include>

<gravity>0 0 -9.81</gravity>

  </world>
</sdf>
  \end{lstlisting}

  
\item We spawned the robot in our new gazebo world modifying the launch file:
 
 \begin{lstlisting}[language=Python]
 	    iiwa_simulation_world = PathJoinSubstitution(
        [FindPackageShare(description_package),
            'gazebo/worlds', 'aruco_world']
    )
    
    declared_arguments.append(
        DeclareLaunchArgument(
            'gz_args',
            default_value=[
                TextSubstitution(text='-r -v 1 '),  
                iiwa_simulation_world               
            ],
            description='Arguments for gz_sim'
        )
    )
\end{lstlisting}
Then in \lstinline|kdl_control| class we added a new vision-based controller called \lstinline|vision_ctrl| , it can be activated from the terminal as well as the other velocity\_ctrl by inserting "vision" in the \lstinline|ctrl| ROS parameter.\\
Then we created a subscribed to the aruco topic \lstinline|/aruco_single/pose| in the source code \lstinline|ros2_kdl_node.cpp|

\begin{lstlisting}[language=C++]
auto qos_profile = rclcpp::QoS(rclcpp::KeepLast(10)).reliable();
MarkerPoseSubscriber_ = this->create_subscription<geometry_msgs::msg::PoseStamped>(
            "/aruco_single/pose", qos_profile, std::bind(&Iiwa_pub_sub::aruco_pose_subscriber, this, std::placeholders::_1));
            
            ...
            
            void aruco_pose_subscriber(const geometry_msgs::msg::PoseStamped& pose_stamped_msg)
    {
        cPo_(0) = pose_stamped_msg.pose.position.x;
        cPo_(1) = pose_stamped_msg.pose.position.y;
        cPo_(2) = pose_stamped_msg.pose.position.z;
    }
\end{lstlisting}
As already said, in \lstinline|kdl_control.cpp| we added the \lstinline|vision_ctrl| class: 

\begin{lstlisting}[language=C++]
KDL::JntArray KDLController::vision_ctrl(int Kp, Eigen::Vector3d cPo,Eigen::Vector3d sd )
{
    unsigned int nj = robot_->getNrJnts();

    Eigen::Matrix<double,3,3> Rc;
    Rc = toEigen(robot_->getEEFrame().M);//assumiamo che la matrice di rotazione siano approssimabili
    Eigen::MatrixXd K(nj,nj);
    K = 3*Kp*K.Identity(nj,nj);

    Eigen::Matrix<double,6,6> R =Eigen::Matrix<double,6,6>::Zero();

    R.block<3, 3>(0, 0) = Rc;
    R.block<3, 3>(3, 3) = Rc;//.transpose();

    Eigen::Vector3d s;
    for (int i=0; i<3; i++){
        s(i) = cPo(i)/cPo.norm();
    }
    
    
    RCLCPP_INFO(rclcpp::get_logger("KDLController"),
                "vector s: %f %f %f",
                s(0),s(1),s(2));
    
    Eigen::Matrix<double,3,3> L1;
    L1 = -1/cPo.norm()* (Eigen::Matrix3d::Identity() - s*s.transpose());

    Eigen::Matrix3d S_skew = Eigen::Matrix3d::Zero();
    S_skew <<     0, -s.z(),  s.y(),
                 s.z(),      0, -s.x(),
                -s.y(),  s.x(),      0;


    Eigen::Matrix<double,3,3> L2;
    L2 = S_skew;

    Eigen::Matrix<double,3,6> L;

    L.block<3, 3>(0, 0) = L1;
    L.block<3, 3>(0, 3) = L2;
    
    L = L*R;

    Eigen::MatrixXd J;
    J = robot_->getEEJacobian().data;
    Eigen::MatrixXd Jc; 
    Jc  = J; //assumiamo che i due jacobiani siano uguali

    
    Eigen::MatrixXd I;
    I = Eigen::MatrixXd::Identity(nj,nj);

    Eigen::MatrixXd JntLimits_ (nj,2);
    JntLimits_ = robot_->getJntLimits();

    Eigen::VectorXd q_min(nj);
    Eigen::VectorXd q_max(nj);
    q_min = JntLimits_.col(0);
    q_max = JntLimits_.col(1);

    Eigen::VectorXd q(nj);
    q  = robot_->getJntValues();

    double lambda = 50;

    Eigen::VectorXd q0_dot(nj);
    for (unsigned int i = 0; i<nj; i++) {
        
        double L =(q_max(i) - q_min(i))*(q_max(i) - q_min(i));

        double G = (2*q(i) - q_max(i) - q_min(i));

        double D = (q_max(i)- q(i))*(q(i)- q_min(i));

        q0_dot(i) = 1/lambda*L*G/(D*D);

    }

    Eigen::MatrixXd N (nj,nj);

    N = I - pseudoinverse(J)*J;
    
    Eigen::MatrixXd J_pinv = pseudoinverse(L*J);
    KDL::JntArray qd(nj);
    qd.data =  K*J_pinv*sd + N * q0_dot;

   
    return qd;
}
\end{lstlisting}


 Since the camera and the end effector are very near, we chose as Jacobian camera, the Jacobian of the end effector.  We did the same for the Rotation matrix, we were aware that they are different joints, but since we are not much interested about the orientation of the camera, with respect to the aruco tag, this approximation can be acceptable. \\
 Then we computer the matrix $R$, the assignment required  that $R$ is 
 $$
 	R = \begin{bmatrix}
 	R_c^T & 0 \\ 0 & R_c^T
 	\end{bmatrix}
 $$
 However we tried to impose this equation, but we noticed that the control performed much better without imposing the Transposition on the $R_c$ matrix. Then we computed the unit-norm axis $s$ and calculated the $L(s)$ matrix. Eventually we imposed that 


 	\[
\dot{\mathbf{q}} = K (L(\mathbf{s}) J_c)^\dagger \mathbf{s_d} + N \dot{\mathbf{q}}_0
\]

Where K is a diagonal matrix and $\mathbf{s}_d = [0,0,1]$ which is the desired pointing direction.

While the current direction is:

\[
\mathbf{s} = \frac{{}^{c}\mathbf{P}{o}}{\|{}^{c}\mathbf{P}{o}\|}
\]


\[
L(\mathbf{s}) = \biggl[-\frac{1}{\|{}^{c}\mathbf{P}_{o}\|} \bigl(\mathbf{I} - \mathbf{s}\mathbf{s}^\top\bigr)\: \mathbf{S}(\mathbf{s}) \biggr] \mathbf{R}
\]
At this \href{https://www.youtube.com/watch?v=kjD034DE2o8}{link} it's possible to see the video of robot following the aruco tag. The plots shown in the video are here retrieved \ref{Vision_control}

\begin{figure}
\includegraphics[width=0.85\linewidth]{img/vel_vision}
\caption{Commanded velocities with Vision Control}
\label{Vision_control}
\end{figure}


\newpage \item We created a ROS 2 service to update the aruco marker position in Gazebo. We started from the \lstinline|/set_pose| ign service and then created a \lstinline|parameter_bridge| in the previously created launch file. 
\newline This is the bridge code:
  \begin{lstlisting}[language=python]
	bridge_set_pose = Node(
        package='ros_gz_bridge',
        executable='parameter_bridge',
        arguments=[
            f'/world/aruco_world/set_pose@ros_gz_interfaces/srv/SetEntityPose@gz.msgs.Pose@gz.msgs.Boolean',
        ],
        remappings=[
            (f'/world/aruco_world/set_pose', '/set_aruco_pose'),
        ],
        output='screen'
   )
\end{lstlisting}

Then we tested the bridged service via ROS 2 service call:
  \begin{lstlisting}[language=bash]
ros2 service call /set_aruco_pose ros_gz_interfaces/srv/SetEntityPose '{"entity": {"name": "aruco_tag", "type": 2}, "pose": {"position": {"x": 1.0, "y": 0.0, "z": 0.5}, "orientation": {"x": 0.0, "y": 0.0, "z": 0.0, "w": 1.0}}}'
\end{lstlisting}

This video shows how it works:
\url{https://youtu.be/Mql9K-V0ZbU}



\end{enumerate}  

 

      
            
              
              
              